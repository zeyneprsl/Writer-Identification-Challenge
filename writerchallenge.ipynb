{"cells":[{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"6bleTa18aDrM","executionInfo":{"status":"error","timestamp":1734627320961,"user_tz":-180,"elapsed":49795,"user":{"displayName":"Zeynep Persil","userId":"03512821932241917078"}},"outputId":"9f277664-bf52-42a8-f3c0-8c01dffd5e45"},"outputs":[{"output_type":"error","ename":"ValueError","evalue":"X has 84608 features, but MaxAbsScaler is expecting 86272 features as input.","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-12-dc3bd8047d90>\u001b[0m in \u001b[0;36m<cell line: 420>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    426\u001b[0m     \u001b[0msc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMaxAbsScaler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m     \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m     \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Test verisinde sadece dönüştürme işlemi\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m     \u001b[0;31m# XGBoost modeli\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/preprocessing/_data.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m   1312\u001b[0m         \u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_namespace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1314\u001b[0;31m         X = validate_data(\n\u001b[0m\u001b[1;32m   1315\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2963\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2964\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mcheck_params\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ensure_2d\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2965\u001b[0;31m         \u001b[0m_check_n_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_estimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2966\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2967\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_check_n_features\u001b[0;34m(estimator, X, reset)\u001b[0m\n\u001b[1;32m   2827\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2828\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mn_features\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_features_in_\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2829\u001b[0;31m         raise ValueError(\n\u001b[0m\u001b[1;32m   2830\u001b[0m             \u001b[0;34mf\"X has {n_features} features, but {estimator.__class__.__name__} \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2831\u001b[0m             \u001b[0;34mf\"is expecting {estimator.n_features_in_} features as input.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mValueError\u001b[0m: X has 84608 features, but MaxAbsScaler is expecting 86272 features as input."]}],"source":["\"\"\"\n","import numpy as np\n","import cv2\n","from sklearn.covariance import EmpiricalCovariance\n","from sklearn.preprocessing import MaxAbsScaler\n","from sklearn.linear_model import LogisticRegression\n","import os\n","from sklearn.model_selection import cross_val_score\n","\n","# Görsel iyileştirme fonksiyonu\n","def improve_image(image):\n","    # Gürültüyü azaltma (Gaussian Blur)\n","    image = cv2.GaussianBlur(image, (5, 5), 0)\n","\n","    # Histogram eşitleme\n","    image = cv2.equalizeHist(image)\n","\n","    # Kontrast artırma (daha gelişmiş bir yöntemle)\n","    # Histogram equalization ile elde edilen görüntü üzerine threshold uygulama\n","    alpha = 1.5  # Kontrast artırma faktörü\n","    beta = 0  # Parlaklık ekleme\n","    image = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n","\n","    return image\n","\n","# Öznitelik çıkarımı fonksiyonu\n","def extract_sift_brief(image):\n","    # Görsel iyileştirme adımını ekleyelim\n","    image = improve_image(image)\n","\n","    detector = cv2.FastFeatureDetector_create()\n","    keypoints = detector.detect(image, None)\n","\n","    sift = cv2.SIFT_create()\n","    keypoints, descriptors = sift.compute(image, keypoints)\n","    cov = EmpiricalCovariance().fit(descriptors).covariance_\n","    cov = cov.ravel()\n","\n","    brief = cv2.xfeatures2d.BriefDescriptorExtractor_create()\n","    keypoints, descriptors = brief.compute(image, keypoints)\n","    cov_1 = EmpiricalCovariance().fit(descriptors).covariance_\n","    cov_1 = cov_1.ravel()\n","\n","    fea = cov.tolist() + cov_1.tolist()\n","    return np.array(fea)\n","\n","# Eğitim verilerini hazırlama\n","def prepare_data(data_dir, extract_func=extract_sift_brief):\n","    X = []\n","    y = []\n","    for fn in os.listdir(data_dir):\n","        sl = fn.split(\".\")\n","        sl_1 = sl[0].split(\"_\")\n","        label = int(sl_1[0])\n","        filename = os.path.join(data_dir, fn)\n","        img = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)\n","\n","        # Görsel iyileştirmeyi burada da ugyladım\n","        img = improve_image(img)\n","\n","        x = extract_func(img)\n","        X.append(x)\n","        y.append(label)\n","    return np.array(X), np.array(y)\n","\n","# Test verilerini hazırlama\n","def prepare_data_test(test_dir, extract_func=extract_sift_brief):\n","    X = []\n","    for fn in os.listdir(test_dir):\n","        filename = os.path.join(test_dir, fn)\n","        img = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)\n","\n","        # Görsel iyileştirmeyi burada da uygulayabiliriz\n","        img = improve_image(img)\n","\n","        x = extract_func(img)\n","        X.append(x)\n","    return X\n","\n","# Tahminleri dosyaya yazma fonksiyonu\n","def write_predictions_to_file(predictions, filename=\"/content/drive/MyDrive/Colab Notebooks/predictions.txt\"):\n","    with open(filename, \"w\") as f:\n","        for pred in predictions:\n","           np.savetxt(filename, predictions, fmt='%d')  # '%d' integer formatında yazdırır\n","\n","\n","if __name__ == \"__main__\":\n","    # Eğitim ve test verilerini hazırlama\n","    X, y = prepare_data(\"/content/drive/MyDrive/Colab Notebooks/wiriter-images/training\")\n","    X_test = prepare_data_test(\"/content/drive/MyDrive/Colab Notebooks/wiriter-images/test\")\n","\n","    # Özellikler üzerinde normalizasyon işlemi\n","    sc = MaxAbsScaler()\n","    X = sc.fit_transform(X)\n","    X_test = sc.transform(X_test)\n","\n","    # Logistic Regression modeli\n","    clf = LogisticRegression(C=0.5)\n","\n","    # Kross-valide doğruluk hesaplaması\n","    cv_scores = cross_val_score(clf, X, y, cv=2)\n","    print(f\"Cross-validation doğruluğu: {np.mean(cv_scores):.4f}\")\n","\n","    # Modeli eğitme\n","    clf.fit(X, y)\n","\n","    # Test verisi üzerinde tahmin yapma\n","    predictions = clf.predict(X_test)\n","\n","    # Tahminleri bir dosyaya yazma\n","    write_predictions_to_file(predictions)\n","    0.9510 cv2 c0.5\n","\"\"\"\n","\n","\"\"\"\n","import numpy as np\n","import cv2\n","from sklearn.covariance import EmpiricalCovariance\n","from sklearn.preprocessing import MaxAbsScaler\n","from sklearn.linear_model import LogisticRegression\n","import os\n","from sklearn.model_selection import cross_val_score\n","\n","# Görsel iyileştirme fonksiyonu\n","def improve_image(image):\n","    # Gürültüyü azaltma (Gaussian Blur)\n","    image = cv2.GaussianBlur(image, (5, 5), 0)\n","\n","    # Histogram eşitleme\n","    image = cv2.equalizeHist(image)\n","\n","    # Kontrast artırma (daha gelişmiş bir yöntemle)\n","    # Histogram equalization ile elde edilen görüntü üzerine threshold uygulama\n","    alpha = 1.5  # Kontrast artırma faktörü\n","    beta = 0  # Parlaklık ekleme\n","    image = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n","\n","    return image\n","\n","# Öznitelik çıkarımı fonksiyonu\n","def extract_sift_brief(image):\n","    # Görsel iyileştirme adımını ekleyelim\n","    image = improve_image(image)\n","\n","    detector = cv2.FastFeatureDetector_create()\n","    keypoints = detector.detect(image, None)\n","\n","    sift = cv2.SIFT_create()\n","    keypoints, descriptors = sift.compute(image, keypoints)\n","    cov = EmpiricalCovariance().fit(descriptors).covariance_\n","    cov = cov.ravel()\n","\n","    brief = cv2.xfeatures2d.BriefDescriptorExtractor_create()\n","    keypoints, descriptors = brief.compute(image, keypoints)\n","    cov_1 = EmpiricalCovariance().fit(descriptors).covariance_\n","    cov_1 = cov_1.ravel()\n","\n","    fea = cov.tolist() + cov_1.tolist()\n","    return np.array(fea)\n","\n","# Eğitim verilerini hazırlama\n","def prepare_data(data_dir, extract_func=extract_sift_brief):\n","    X = []\n","    y = []\n","    for fn in os.listdir(data_dir):\n","        sl = fn.split(\".\")\n","        sl_1 = sl[0].split(\"_\")\n","        label = int(sl_1[0])\n","        filename = os.path.join(data_dir, fn)\n","        img = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)\n","\n","        # Görsel iyileştirmeyi burada da ugyladım\n","        img = improve_image(img)\n","\n","        x = extract_func(img)\n","        X.append(x)\n","        y.append(label)\n","    return np.array(X), np.array(y)\n","\n","\n","# Test verilerini hazırlama\n","def prepare_data_test(test_dir, extract_func=extract_sift_brief):\n","    X = []\n","    for fn in sorted(os.listdir(test_dir)):\n","        filename = os.path.join(test_dir, fn)\n","        img = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)\n","\n","        # Görsel iyileştirmeyi burada da uygulayabiliriz\n","        img = improve_image(img)\n","\n","        x = extract_func(img)\n","        X.append(x)\n","    return X\n","\n","# Tahminleri dosyaya yazma fonksiyonu\n","def write_predictions_to_file(predictions, filename=\"/content/drive/MyDrive/Colab Notebooks/predictions2.txt\"):\n","    with open(filename, \"w\") as f:\n","        for pred in predictions:\n","           np.savetxt(filename, predictions, fmt='%d')  # '%d' integer formatında yazdırır\n","\n","\n","if __name__ == \"__main__\":\n","    # Eğitim ve test verilerini hazırlama\n","    X, y = prepare_data(\"/content/drive/MyDrive/Colab Notebooks/wiriter-images/training\")\n","    X_test = prepare_data_test(\"/content/drive/MyDrive/Colab Notebooks/wiriter-images/test\")\n","\n","    # Özellikler üzerinde normalizasyon işlemi\n","    sc = MaxAbsScaler()\n","    X = sc.fit_transform(X)\n","    X_test = sc.transform(X_test)\n","\n","    # Logistic Regression modeli\n","    clf = LogisticRegression(C=0.3)\n","\n","    # Kross-valide doğruluk hesaplaması\n","    cv_scores = cross_val_score(clf, X, y, cv=2)\n","    print(f\"Cross-validation doğruluğu: {np.mean(cv_scores):.4f}\")\n","\n","    # Modeli eğitme\n","    clf.fit(X, y)\n","\n","    # Test verisi üzerinde tahmin yapma\n","    predictions = clf.predict(X_test)\n","\n","    # Tahminleri bir dosyaya yazma\n","    write_predictions_to_file(predictions)\n","   # 0.9559->>> hocanın sisteminde 0.93\n","\"\"\"\n","\"\"\"\n","import numpy as np\n","import cv2\n","from sklearn.covariance import EmpiricalCovariance\n","from sklearn.preprocessing import MaxAbsScaler\n","from sklearn.linear_model import LogisticRegression\n","import os\n","from sklearn.model_selection import cross_val_score\n","\n","# Görsel iyileştirme fonksiyonu (CLAHE kullanan versiyon)\n","def improve_image(image):\n","    # Gürültüyü azaltma (Gaussian Blur)\n","    image = cv2.GaussianBlur(image, (5, 5), 0)\n","\n","    # CLAHE (Contrast Limited Adaptive Histogram Equalization)\n","    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n","    image = clahe.apply(image)\n","\n","    # Kontrast artırma\n","    alpha = 1.5  # Kontrast artırma faktörü\n","    beta = 0  # Parlaklık ekleme\n","    image = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n","\n","    return image\n","\n","# Öznitelik çıkarımı fonksiyonu\n","def extract_sift_brief(image):\n","    # Görsel iyileştirme adımını ekleyelim\n","    image = improve_image(image)\n","\n","    detector = cv2.FastFeatureDetector_create()\n","    keypoints = detector.detect(image, None)\n","\n","    sift = cv2.SIFT_create()\n","    keypoints, descriptors = sift.compute(image, keypoints)\n","    cov = EmpiricalCovariance().fit(descriptors).covariance_\n","    cov = cov.ravel()\n","\n","    brief = cv2.xfeatures2d.BriefDescriptorExtractor_create()\n","    keypoints, descriptors = brief.compute(image, keypoints)\n","    cov_1 = EmpiricalCovariance().fit(descriptors).covariance_\n","    cov_1 = cov_1.ravel()\n","\n","    fea = cov.tolist() + cov_1.tolist()\n","    return np.array(fea)\n","\n","# Eğitim verilerini hazırlama\n","def prepare_data(data_dir, extract_func=extract_sift_brief):\n","    X = []\n","    y = []\n","    for fn in os.listdir(data_dir):\n","        sl = fn.split(\".\")\n","        sl_1 = sl[0].split(\"_\")\n","        label = int(sl_1[0])\n","        filename = os.path.join(data_dir, fn)\n","        img = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)\n","\n","        # Görsel iyileştirmeyi burada da uygula\n","        img = improve_image(img)\n","\n","        x = extract_func(img)\n","        X.append(x)\n","        y.append(label)\n","    return np.array(X), np.array(y)\n","\n","# Test verilerini hazırlama\n","def prepare_data_test(test_dir, extract_func=extract_sift_brief):\n","    X = []\n","    for fn in sorted(os.listdir(test_dir)):\n","        filename = os.path.join(test_dir, fn)\n","        img = cv2.imread(filename, cv2.IMREAD_GRAYSCALE)\n","\n","        # Görsel iyileştirmeyi burada da uygula\n","        img = improve_image(img)\n","\n","        x = extract_func(img)\n","        X.append(x)\n","    return X\n","\n","# Tahminleri dosyaya yazma fonksiyonu\n","def write_predictions_to_file(predictions, filename=\"/content/drive/MyDrive/Colab Notebooks/predictions3.txt\"):\n","    with open(filename, \"w\") as f:\n","        for pred in predictions:\n","            np.savetxt(filename, predictions, fmt='%d')  # '%d' integer formatında yazdırır\n","\n","if __name__ == \"__main__\":\n","    # Eğitim ve test verilerini hazırlama\n","    X, y = prepare_data(\"/content/drive/MyDrive/Colab Notebooks/wiriter-images/training\")\n","    X_test = prepare_data_test(\"/content/drive/MyDrive/Colab Notebooks/wiriter-images/test\")\n","\n","    # Özellikler üzerinde normalizasyon işlemi\n","    sc = MaxAbsScaler()\n","    X = sc.fit_transform(X)\n","    X_test = sc.transform(X_test)\n","\n","    # Logistic Regression modeli\n","    clf = LogisticRegression(C=0.3)\n","\n","    # Kross-valide doğruluk hesaplaması\n","    cv_scores = cross_val_score(clf, X, y, cv=2)\n","    print(f\"Cross-validation doğruluğu: {np.mean(cv_scores):.4f}\")\n","\n","    # Modeli eğitme\n","    clf.fit(X, y)\n","\n","    # Test verisi üzerinde tahmin yapma\n","    predictions = clf.predict(X_test)\n","\n","    # Tahminleri bir dosyaya yazma\n","    write_predictions_to_file(predictions)\n","    ###0.9632 hocada 9336\n","\"\"\"\n"]}],"metadata":{"colab":{"provenance":[],"mount_file_id":"1jLtUIus36EFAAejT4GiHaT_GMRkYHNwT","authorship_tag":"ABX9TyPWEjYYwx+nvKfw0q9FQwb7"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}